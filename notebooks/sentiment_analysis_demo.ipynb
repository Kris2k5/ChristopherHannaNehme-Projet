{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Sentiment Analysis - Interactive Demo\n",
    "\n",
    "This notebook demonstrates the complete sentiment analysis system with step-by-step examples.\n",
    "\n",
    "**Author**: Christopher Hanna Nehme  \n",
    "**Date**: January 2024  \n",
    "**Version**: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Load Sample Data](#load-data)\n",
    "3. [Text Preprocessing](#preprocessing)\n",
    "4. [Sentiment Analysis](#analysis)\n",
    "5. [Visualization](#visualization)\n",
    "6. [Alert System](#alerts)\n",
    "7. [Complete Pipeline Example](#pipeline)\n",
    "8. [Custom Analysis](#custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports <a id='setup'></a>\n",
    "\n",
    "First, let's import all required libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Our custom modules\n",
    "from sentiment_analyzer import SentimentAnalyzer, quick_analyze\n",
    "from data_preprocessing import TextPreprocessor, preprocess_text\n",
    "from visualization import SentimentVisualizer\n",
    "from alert_system import SentimentAlertSystem, quick_alert_check\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"‚úì All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data <a id='load-data'></a>\n",
    "\n",
    "Let's load the sample social media dataset containing 100+ posts from various platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "data_path = Path.cwd().parent / 'data' / 'sample_social_media.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Loaded {len(df)} social media posts\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nPlatforms: {df['platform'].unique()}\")\n",
    "print(f\"\\nDate range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample Posts:\")\n",
    "print(\"=\"*80)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing <a id='preprocessing'></a>\n",
    "\n",
    "Before analysis, we clean and normalize the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Example: Clean a single text\n",
    "sample_text = \"OMG @user this is AMAZING!!! https://example.com #love üòç\"\n",
    "cleaned = preprocessor.clean_text(sample_text)\n",
    "\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Cleaned: \", cleaned)\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the entire dataset\n",
    "df_processed = preprocessor.preprocess_dataframe(df, text_column='text')\n",
    "\n",
    "# Get preprocessing statistics\n",
    "stats = preprocessor.get_statistics(df, text_column='text')\n",
    "\n",
    "print(\"Preprocessing Statistics:\")\n",
    "print(\"=\"*80)\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key:20s}: {value}\")\n",
    "\n",
    "# Show comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Before and After Preprocessing (Sample):\")\n",
    "print(\"=\"*80)\n",
    "comparison = pd.DataFrame({\n",
    "    'Original': df.head(5)['text'].values,\n",
    "    'Cleaned': df_processed.head(5)['cleaned_text'].values\n",
    "})\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis <a id='analysis'></a>\n",
    "\n",
    "Now let's analyze sentiment using VADER algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Quick Analysis Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick analyze some example texts\n",
    "examples = [\n",
    "    \"This product is absolutely amazing! Love it! üòç\",\n",
    "    \"Terrible quality. Very disappointed. ÔøΩÔøΩ\",\n",
    "    \"It's okay. Nothing special.\",\n",
    "    \"Best purchase EVER!!! So happy!!!\",\n",
    "    \"Worst experience. Don't buy this garbage.\"\n",
    "]\n",
    "\n",
    "print(\"Quick Sentiment Analysis Examples:\")\n",
    "print(\"=\"*80)\n",
    "for text in examples:\n",
    "    sentiment, score = quick_analyze(text)\n",
    "    print(f\"Text: {text[:60]}...\")\n",
    "    print(f\"Sentiment: {sentiment:8s} | Score: {score:+.4f}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Batch Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer\n",
    "analyzer = SentimentAnalyzer(use_preprocessing=False)  # Already preprocessed\n",
    "\n",
    "# Analyze all posts\n",
    "results = analyzer.analyze_dataframe(df, text_column='text')\n",
    "\n",
    "print(f\"Analyzed {len(results)} posts\")\n",
    "print(\"\\nColumns added:\")\n",
    "print(\"- compound_score: Overall sentiment (-1 to 1)\")\n",
    "print(\"- positive_score: Positive component (0 to 1)\")\n",
    "print(\"- neutral_score: Neutral component (0 to 1)\")\n",
    "print(\"- negative_score: Negative component (0 to 1)\")\n",
    "print(\"- sentiment: Classification (Positive/Negative/Neutral)\")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample Results:\")\n",
    "print(\"=\"*80)\n",
    "results[['text', 'platform', 'sentiment', 'compound_score']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comprehensive summary\n",
    "summary = analyzer.get_sentiment_summary(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SENTIMENT ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal Posts Analyzed: {summary['total_analyzed']}\")\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(f\"  Positive: {summary['positive_count']:3d} ({summary['positive_percentage']:.1f}%)\")\n",
    "print(f\"  Negative: {summary['negative_count']:3d} ({summary['negative_percentage']:.1f}%)\")\n",
    "print(f\"  Neutral:  {summary['neutral_count']:3d} ({summary['neutral_percentage']:.1f}%)\")\n",
    "print(\"\\nAverage Scores:\")\n",
    "print(f\"  Compound:  {summary['avg_compound_score']:+.4f}\")\n",
    "print(f\"  Positive:  {summary['avg_positive_score']:.4f}\")\n",
    "print(f\"  Negative:  {summary['avg_negative_score']:.4f}\")\n",
    "print(f\"  Neutral:   {summary['avg_neutral_score']:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Top Positive and Negative Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top positive posts\n",
    "top_positive = analyzer.get_top_sentiments(results, sentiment_type='positive', n=5)\n",
    "\n",
    "print(\"Top 5 Most Positive Posts:\")\n",
    "print(\"=\"*80)\n",
    "for idx, row in top_positive.iterrows():\n",
    "    print(f\"Score: {row['compound_score']:+.4f} | {row['text'][:70]}...\")\n",
    "    print(f\"Platform: {row['platform']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top negative posts\n",
    "top_negative = analyzer.get_top_sentiments(results, sentiment_type='negative', n=5)\n",
    "\n",
    "print(\"\\nTop 5 Most Negative Posts:\")\n",
    "print(\"=\"*80)\n",
    "for idx, row in top_negative.iterrows():\n",
    "    print(f\"Score: {row['compound_score']:+.4f} | {row['text'][:70]}...\")\n",
    "    print(f\"Platform: {row['platform']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Platform-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze by platform\n",
    "platform_stats = analyzer.analyze_by_platform(results)\n",
    "\n",
    "print(\"\\nSentiment by Platform:\")\n",
    "print(\"=\"*80)\n",
    "platform_stats[['platform', 'positive_percentage', 'negative_percentage', \n",
    "                'neutral_percentage', 'avg_compound_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization <a id='visualization'></a>\n",
    "\n",
    "Create compelling visualizations of the sentiment analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = SentimentVisualizer()\n",
    "\n",
    "print(\"Generating visualizations...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Sentiment Distribution Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment distribution pie chart\n",
    "visualizer.plot_sentiment_distribution(results, show=True)\n",
    "print(\"‚úì Sentiment distribution chart created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Sentiment Scores Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram of sentiment scores\n",
    "visualizer.plot_sentiment_scores_histogram(results, show=True)\n",
    "print(\"‚úì Sentiment scores histogram created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series plot\n",
    "visualizer.plot_time_series(results, timestamp_column='timestamp', show=True)\n",
    "print(\"‚úì Time series trend chart created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Platform Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create platform comparison chart\n",
    "visualizer.plot_platform_comparison(results, show=True)\n",
    "print(\"‚úì Platform comparison chart created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for positive and negative sentiments\n",
    "positive_texts = results[results['sentiment'] == 'Positive']['text'].tolist()\n",
    "negative_texts = results[results['sentiment'] == 'Negative']['text'].tolist()\n",
    "\n",
    "if positive_texts:\n",
    "    visualizer.create_wordcloud(positive_texts, sentiment_type='positive', show=True)\n",
    "    print(\"‚úì Positive sentiment word cloud created\")\n",
    "\n",
    "if negative_texts:\n",
    "    visualizer.create_wordcloud(negative_texts, sentiment_type='negative', show=True)\n",
    "    print(\"‚úì Negative sentiment word cloud created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Generate Complete Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive HTML dashboard\n",
    "dashboard_path = visualizer.generate_dashboard(results, summary)\n",
    "print(f\"\\n‚úì Dashboard generated: {dashboard_path}\")\n",
    "print(\"\\nOpen the dashboard in your browser to see all visualizations together!\")\n",
    "\n",
    "# Display dashboard link\n",
    "from IPython.display import HTML, display\n",
    "display(HTML(f'<a href=\"{dashboard_path}\" target=\"_blank\">Open Dashboard</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Alert System <a id='alerts'></a>\n",
    "\n",
    "Detect negative sentiment spikes and generate alerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize alert system\n",
    "alert_system = SentimentAlertSystem()\n",
    "\n",
    "# Quick alert check\n",
    "has_alerts, message = quick_alert_check(results)\n",
    "print(\"Quick Alert Check:\")\n",
    "print(\"=\"*80)\n",
    "print(message)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 High Priority Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag high priority negative comments\n",
    "high_priority = alert_system.flag_high_priority_comments(results)\n",
    "\n",
    "print(f\"\\nHigh Priority Negative Comments: {len(high_priority)}\")\n",
    "print(\"=\"*80)\n",
    "if len(high_priority) > 0:\n",
    "    print(\"\\nTop 10 Most Critical:\")\n",
    "    high_priority[['text', 'compound_score', 'platform']].head(10)\n",
    "else:\n",
    "    print(\"No high priority comments detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Comprehensive Alert Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive alert report\n",
    "alert_report = alert_system.generate_alert_report(results)\n",
    "\n",
    "print(\"\\nAlert Report Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total Alerts: {alert_report['alert_count']}\")\n",
    "print(f\"Critical Alerts: {alert_report['has_critical_alerts']}\")\n",
    "print(f\"High Priority Comments: {alert_report.get('high_priority_count', 0)}\")\n",
    "\n",
    "if alert_report['alerts']:\n",
    "    print(\"\\nAlert Details:\")\n",
    "    print(\"-\"*80)\n",
    "    for alert in alert_report['alerts']:\n",
    "        print(f\"[{alert['severity']}] {alert['alert_type']}\")\n",
    "        print(f\"  {alert['message']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Alert Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create human-readable alert summary\n",
    "alert_summary = alert_system.create_alert_summary(alert_report)\n",
    "print(alert_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Pipeline Example <a id='pipeline'></a>\n",
    "\n",
    "Putting it all together in a complete workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_social_media(csv_path, generate_visuals=True, check_alerts=True):\n",
    "    \"\"\"\n",
    "    Complete sentiment analysis pipeline.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to CSV file with social media data\n",
    "        generate_visuals: Whether to generate visualizations\n",
    "        check_alerts: Whether to check for alerts\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with all results\n",
    "    \"\"\"\n",
    "    print(\"Starting sentiment analysis pipeline...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Load data\n",
    "    print(\"1. Loading data...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"   Loaded {len(df)} posts\")\n",
    "    \n",
    "    # 2. Analyze sentiment\n",
    "    print(\"2. Analyzing sentiment...\")\n",
    "    analyzer = SentimentAnalyzer()\n",
    "    results = analyzer.analyze_dataframe(df)\n",
    "    summary = analyzer.get_sentiment_summary(results)\n",
    "    print(f\"   Analysis complete: {summary['positive_percentage']:.1f}% positive, \"\n",
    "          f\"{summary['negative_percentage']:.1f}% negative\")\n",
    "    \n",
    "    # 3. Generate visualizations\n",
    "    dashboard_path = None\n",
    "    if generate_visuals:\n",
    "        print(\"3. Generating visualizations...\")\n",
    "        visualizer = SentimentVisualizer()\n",
    "        dashboard_path = visualizer.generate_dashboard(results, summary)\n",
    "        print(f\"   Dashboard created: {dashboard_path}\")\n",
    "    \n",
    "    # 4. Check alerts\n",
    "    alert_report = None\n",
    "    if check_alerts:\n",
    "        print(\"4. Checking for alerts...\")\n",
    "        alert_system = SentimentAlertSystem()\n",
    "        alert_report = alert_system.generate_alert_report(results)\n",
    "        print(f\"   Found {alert_report['alert_count']} alerts\")\n",
    "    \n",
    "    # 5. Save results\n",
    "    print(\"5. Saving results...\")\n",
    "    output_path = csv_path.replace('.csv', '_analyzed.csv')\n",
    "    results.to_csv(output_path, index=False)\n",
    "    print(f\"   Results saved: {output_path}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"‚úì Pipeline complete!\")\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'summary': summary,\n",
    "        'dashboard': dashboard_path,\n",
    "        'alerts': alert_report\n",
    "    }\n",
    "\n",
    "# Run the complete pipeline\n",
    "pipeline_results = analyze_social_media(\n",
    "    csv_path=str(Path.cwd().parent / 'data' / 'sample_social_media.csv'),\n",
    "    generate_visuals=True,\n",
    "    check_alerts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Analysis <a id='custom'></a>\n",
    "\n",
    "Try analyzing your own custom text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive custom text analysis\n",
    "def analyze_custom_text(text):\n",
    "    \"\"\"\n",
    "    Analyze sentiment of custom text.\n",
    "    \"\"\"\n",
    "    analyzer = SentimentAnalyzer()\n",
    "    scores = analyzer.analyze_text(text)\n",
    "    sentiment = analyzer.classify_sentiment(scores['compound'])\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TEXT SENTIMENT ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nInput Text:\\n{text}\")\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"\\nSentiment: {sentiment}\")\n",
    "    print(f\"\\nScores:\")\n",
    "    print(f\"  Compound:  {scores['compound']:+.4f}\")\n",
    "    print(f\"  Positive:  {scores['pos']:.4f}\")\n",
    "    print(f\"  Neutral:   {scores['neu']:.4f}\")\n",
    "    print(f\"  Negative:  {scores['neg']:.4f}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Visual representation\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    colors = {'Positive': '#2ecc71', 'Neutral': '#95a5a6', 'Negative': '#e74c3c'}\n",
    "    ax.barh(['Compound'], [scores['compound']], color=colors[sentiment])\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "    ax.axvline(x=0.05, color='green', linestyle='--', alpha=0.5)\n",
    "    ax.axvline(x=-0.05, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Sentiment Score')\n",
    "    ax.set_title(f'Sentiment: {sentiment} ({scores[\"compound\"]:+.4f})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Try some examples\n",
    "custom_examples = [\n",
    "    \"I absolutely love this new feature! It's a game changer! üéâ\",\n",
    "    \"This is the worst product I've ever used. Complete waste of money.\",\n",
    "    \"The interface is okay, but needs some improvements.\"\n",
    "]\n",
    "\n",
    "for example in custom_examples:\n",
    "    analyze_custom_text(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Your Own Text\n",
    "\n",
    "Modify the text below and run the cell to analyze your own content!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your own text here\n",
    "my_text = \"This sentiment analysis system is fantastic! It works really well!\"\n",
    "\n",
    "analyze_custom_text(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "‚úÖ **Data Loading**: Loading and exploring social media data  \n",
    "‚úÖ **Preprocessing**: Cleaning and normalizing text  \n",
    "‚úÖ **Sentiment Analysis**: Using VADER to classify sentiment  \n",
    "‚úÖ **Visualization**: Creating comprehensive charts and dashboards  \n",
    "‚úÖ **Alert System**: Detecting negative sentiment spikes  \n",
    "‚úÖ **Complete Pipeline**: End-to-end automated workflow  \n",
    "‚úÖ **Custom Analysis**: Analyzing your own text  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try analyzing your own social media data\n",
    "- Adjust sentiment thresholds in `src/config.py`\n",
    "- Experiment with different alert settings\n",
    "- Extend the system with additional features\n",
    "- Deploy to production environment\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [VADER Documentation](https://github.com/cjhutto/vaderSentiment)\n",
    "- [Project README](../README.md)\n",
    "- [Product Specification](../product_specification.md)\n",
    "- [Presentation Guide](../docs/presentation_guide.md)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Analyzing!** üéØüìä"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
